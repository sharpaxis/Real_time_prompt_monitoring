


# Real-Time Prompt Monitoring System

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/build-passing-brightgreen" /></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue" /></a>
  <a href="https://huggingface.co/Sharpaxis/Llama-2-7_Ethical_Guardian"><img src="https://img.shields.io/badge/Model-Llama_2_Ethical_Guardian-orange" /></a>
  <a href="https://huggingface.co/Sharpaxis/distilbert-sensitive-classification"><img src="https://img.shields.io/badge/Model-DistilBERT_Sensitive-red" /></a>
  <a href="https://huggingface.co/datasets/Sharpaxis/Synthetic-Sensitive-data"><img src="https://img.shields.io/badge/Dataset-Synthetic_Sensitive_Data-yellow" /></a>
</p>



# Overview

The Real-Time Prompt Monitoring System is designed for companies that use chatbots internally and want to ensure that employee-entered prompts remain safe, compliant, and free from sensitive information leaks.

The system processes every prompt in real-time and performs two major checks:
	â€¢	Hate/Toxic Speech Detection
	â€¢	Sensitive Information Leak Detection
(passwords, API keys, emails, tokens, credentials, and other confidential identifiers)

This solution acts as an AI firewall between employees and corporate LLM systems.



# âœ¨ Features

ðŸ”’ Private Information Leak Detection

Powered by a fine-tuned Llama 2 model:
âž¡ Ethical Guardian v2
### ðŸ”— https://huggingface.co/Sharpaxis/Llama-2-7_Ethical_Guardian

Detects:
	â€¢	API keys
	â€¢	Passwords
	â€¢	Access tokens
	â€¢	Internal IDs
	â€¢	Sensitive corporate data
	â€¢	Personally Identifiable Information (PII)

## ðŸ›¡ Harmful Speech Detection

Using ToxicBERT, the system detects and blocks:
	â€¢	Abusive language
	â€¢	Harassment
	â€¢	Discriminatory or toxic sentences

## ðŸ§ª Sensitive-String Classifier (DistilBERT)

Model:
### âž¡ https://huggingface.co/Sharpaxis/distilbert-sensitive-classification

This model specifically detects:
	â€¢	Keys
	â€¢	Passwords
	â€¢	API tokens
	â€¢	Private strings that follow known credential patterns

Works as a second-layer safety filter.


## ðŸ§° Technology Stack

Component	Description
Python	Primary backend language
Flask	API server for real-time inference
ToxicBERT	Toxic speech detection
Llama-2 (Ethical Guardian v2)	Sensitive information leak detection
DistilBERT Sensitive Classifier	Detects credential-pattern leaks
Google Colab	Used for fine-tuning Llama 2
QLoRA	Parameter-efficient training methodology



## ðŸš€ Installation

1. Clone the Repository

git clone https://github.com/your-username/prompt-monitoring-system.git
cd prompt-monitoring-system

2. Create Virtual Environment

python3 -m venv venv
source venv/bin/activate     # macOS & Linux
venv\Scripts\activate        # Windows

3. Install Dependencies

pip install -r requirements.txt

4. Run the Application

flask run


# ðŸ¤– Model Information

## ðŸ¦™ Ethical Guardian v2 â€” Llama-2 Fine-Tuned Model

Property	Value
Base Model	NousResearch/Llama-2-7b-chat-hf
Dataset	synapsecai/synthetic-sensitive-information (20%)
QLoRA Rank	64
QLoRA Alpha	16
Dropout	0.1
Quantization	4-bit NF4
Training Epochs	1 epoch
Batch Size	32
Learning Rate	2e-4
Optimizer	paged_adamw_32bit
Max Grad Norm	0.3
Warmup	3%
Gradient Checkpointing	Enabled

DistilBERT Sensitive Classifier

Detects:
	â€¢	Passwords
	â€¢	API Keys
	â€¢	Tokens
	â€¢	Other credential strings

### ðŸ”— https://huggingface.co/Sharpaxis/distilbert-sensitive-classification

ToxicBERT

Used for filtering:
	â€¢	Toxic, harmful, or unsafe text
	â€¢	Threatening/abusive content


# ðŸ“Š Dataset

The primary dataset used for fine-tuning:

ðŸ”— Synthetic Sensitive Data
### https://huggingface.co/datasets/Sharpaxis/Synthetic-Sensitive-data

Contains examples of:
	â€¢	API keys
	â€¢	Passwords
	â€¢	Database credentials
	â€¢	PII samples
	â€¢	Sensitive corporate identifiers

